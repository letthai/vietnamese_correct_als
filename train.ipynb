{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from lion_pytorch import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# A list of all multilingual tokenizer which require lang attribute.\n",
    "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"bmd1905/vietnamese-correction\",\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "    resize_position_embeddings: Optional[bool] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n",
    "            \"the model's position embeddings.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    lang: str = field(default=\"vi\", metadata={\"help\": \"Language id for summarization.\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    text_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
    "    )\n",
    "    summary_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input evaluation data file to evaluate the metrics (rouge) on \"\n",
    "            \"(a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on \" \"(a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=256,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=256,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "            \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "            \"during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "            \"which is used during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
    "    )\n",
    "\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The token to force as the first generated token after the decoder_start_token_id.\"\n",
    "            \"Useful for multilingual models like mBART where the first generated token\"\n",
    "            \"needs to be the target language token (Usually it is the target language token)\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(model_name_or_path='bmd1905/vietnamese-correction')\n",
    "\n",
    "data_args = DataTrainingArguments(dataset_name='D:/HMI_Lab/vietnamese-correct-als/vi.test.csv',\n",
    "                                  preprocessing_num_workers=8,\n",
    "                                  max_source_length=256,\n",
    "                                  max_target_length=256)\n",
    "# Cài đặt các siêu tham số huấn luyện\n",
    "training_args = Seq2SeqTrainingArguments(output_dir=\"./models/my-vietnamese-correct-als\",\n",
    "                                         learning_rate=1e-5,\n",
    "                                         per_device_train_batch_size=1,\n",
    "                                         per_device_eval_batch_size=64,\n",
    "                                         gradient_accumulation_steps=1,\n",
    "                                         num_train_epochs=4,\n",
    "                                         logging_steps=100,\n",
    "                                         save_steps=500,\n",
    "                                         overwrite_output_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        print(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải mô hình và tokenizer\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "config.attention_dropout = 0.2\n",
    "config.classifier_dropout = 0.4\n",
    "config.dropout = 0.2\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
    "    if isinstance(tokenizer, MBartTokenizer):\n",
    "        model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.lang]\n",
    "    else:\n",
    "        model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.lang)\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "if (\n",
    "    hasattr(model.config, \"max_position_embeddings\")\n",
    "    and model.config.max_position_embeddings < data_args.max_source_length\n",
    "):\n",
    "    if model_args.resize_position_embeddings is None:\n",
    "        print(\n",
    "            f\"Increasing the model's number of position embedding vectors from {model.config.max_position_embeddings} \"\n",
    "            f\"to {data_args.max_source_length}.\"\n",
    "        )\n",
    "        model.resize_position_embeddings(data_args.max_source_length)\n",
    "    elif model_args.resize_position_embeddings:\n",
    "        model.resize_position_embeddings(data_args.max_source_length)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has {model.config.max_position_embeddings}\"\n",
    "            f\" position encodings. Consider either reducing `--max_source_length` to {model.config.max_position_embeddings} or to automatically \"\n",
    "            \"resize the model's position encodings by passing `--resize_position_embeddings`.\"\n",
    "        )\n",
    "\n",
    "prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn bị dữ liệu huấn luyện\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": data_args.dataset_name},\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "column_name = train_dataset.column_names\n",
    "\n",
    "# Get the column names for input/target.\n",
    "column_names = train_dataset.column_names\n",
    "if data_args.text_column is None:\n",
    "    error_column = column_names[0]\n",
    "else:\n",
    "    error_column = data_args.text_column\n",
    "\n",
    "    if error_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if data_args.summary_column is None:\n",
    "    correct_column = column_names[1]\n",
    "else:\n",
    "    correct_column = data_args.summary_column\n",
    "    if correct_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "# Temporarily set max_target_length for training.\n",
    "max_target_length = data_args.max_target_length\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # remove pairs where at least one record is None\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[error_column])):\n",
    "        if examples[error_column][i] is not None and examples[correct_column][i] is not None:\n",
    "            inputs.append(examples[error_column][i])\n",
    "            targets.append(examples[correct_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "if data_args.max_train_samples is not None:\n",
    "    max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    ")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"cer\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if data_args.ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    return {\"cer\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "# Slice the DataFrame\n",
    "k = 1\n",
    "subset_start = len(df) * k // 100\n",
    "subset_end = len(df) * (k + 10) // 100\n",
    "subset_df = df.iloc[subset_start:subset_end]\n",
    "\n",
    "val_ds = Dataset.from_pandas(subset_df)\n",
    "print(val_ds)\n",
    "val_tokenized_datasets  = val_ds.map(preprocess_function, batched=True)\n",
    "print(val_tokenized_datasets[:1])\n",
    "\n",
    "# Calculate the slices\n",
    "slices = [(slice(0, k * len(df) // 100), slice((k + 10) * len(df) // 100, None))]\n",
    "\n",
    "# Apply slicing to generate new DataFrames\n",
    "cut_dfs = [pd.concat([df.iloc[s1], df.iloc[s2]]) for s1, s2 in slices]\n",
    "# Concatenate all DataFrames in cut_dfs into a single DataFrame\n",
    "new_df = pd.concat(cut_dfs)\n",
    "train_ds = Dataset.from_pandas(new_df)\n",
    "print(train_ds)\n",
    "train_tokenized_datasets = train_ds.map(preprocess_function, batched=True)\n",
    "print(train_tokenized_datasets[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for the first train\n",
    "# dataset = load_dataset(\n",
    "#     \"csv\",\n",
    "#     data_files={\"train\": data_args.dataset_name},\n",
    "#     cache_dir=model_args.cache_dir,\n",
    "#     use_auth_token=True if model_args.use_auth_token else None,\n",
    "#     )\n",
    "\n",
    "# train_test_dataset = dataset[\"train\"].train_test_split(test_size=0.012)\n",
    "# test_valid = train_test_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# train_dataset = preprocess_function(train_test_dataset['train'])\n",
    "# test_dataset = preprocess_function(test_valid['test'])\n",
    "# val_dataset = preprocess_function(test_valid['train'])\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "# Training\n",
    "# checkpoint = None\n",
    "# if training_args.resume_from_checkpoint is not None:\n",
    "#     checkpoint = training_args.resume_from_checkpoint\n",
    "# elif last_checkpoint is not None:\n",
    "#     checkpoint = last_checkpoint\n",
    "# train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "# trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "# metrics = train_result.metrics\n",
    "# max_train_samples = (\n",
    "#     data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "# )\n",
    "# metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "# trainer.log_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"train\", metrics)\n",
    "# trainer.save_state()\n",
    "\n",
    "# # Evaluation\n",
    "# results = {}\n",
    "# max_length = (\n",
    "#     training_args.generation_max_length\n",
    "#     if training_args.generation_max_length is not None\n",
    "#     else data_args.val_max_target_length\n",
    "# )\n",
    "# num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
    "# metrics_val = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
    "# max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(val_dataset)\n",
    "# metrics[\"val_samples\"] = min(max_eval_samples, len(val_dataset))\n",
    "# print(metrics_val)\n",
    "\n",
    "# # Predict\n",
    "# predict_results = trainer.predict(\n",
    "#     predict_dataset, metric_key_prefix=\"predict\", max_length=max_length, num_beams=num_beams\n",
    "# )\n",
    "# metrics_pre = predict_results.metrics\n",
    "# max_predict_samples = (\n",
    "#     data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "# )\n",
    "# metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "# print(metrics_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=val_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "max_length = (\n",
    "    training_args.generation_max_length\n",
    "    if training_args.generation_max_length is not None\n",
    "    else data_args.val_max_target_length\n",
    ")\n",
    "num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
    "print(\"*** Evaluate ***\")\n",
    "metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
    "max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(val_tokenized_datasets)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(val_tokenized_datasets))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = trainer.predict(val_tokenized_datasets, max_length=max_length, num_beams=num_beams)\n",
    "metrics = predict_results.metrics\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "corrector = pipeline(\"text2text-generation\", model=\"D:\\\\HMI_Lab\\\\vietnamese-correct-als\\\\models\\\\my-vietnamese-correct-als\\\\checkpoint-1000\", tokenizer=tokenizer)\n",
    "\n",
    "print(corrector(\"oke\", max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corrector(\"tôdi đau lừng và muốnd đi tiểud\", max_length=256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
